# Authenticityâ€‘Driven Disaster Tweet Classification (2025â€‘Spring DM)


* **Team Members**  
  - Lulin Yang   
  - Wendi Li  
  - Xiaoxuan Qin  
* **Project presentation**: 
  <a href="docs/presentation_slides.pdf" target="_blank" rel="noopener">
    ğŸ–¥ï¸ View Presentation Slides
  </a>

* **Project paper**: 
  <a href="docs/report_paper.pdf" target="_blank" rel="noopener">
    ğŸ–¥ï¸ View Report Paper
  </a>


## Description
This project detects and classifies disaster-related tweets on Twitter. We clean and preprocess raw tweet text, then extract features using Bagâ€‘ofâ€‘Words, TFâ€‘IDF without and with PCA, GloVe word embeddings, and BERT sentence embeddings. We train Logistic Regression, SVM, and Neural Network models, evaluate them with Accuracy, Precision, Recall, F1â€‘score, and AUC, and perform error analysis on false positives. Finally, we apply unsupervised clustering to distinguish subtypes of detected emergency tweets.


## Prerequisites
### System
- **R**Â â‰¥Â 4.0  
- **Python**Â â‰¥Â 3.7

### R packages

```r
install.packages(c(
  ## 1. Data Manipulation
  "tidyverse",    # dplyr, tidyr, etc. for data wrangling
  "data.table",   # superâ€fast data frames
  
  ## 2. Text Preprocessing
  "tidytext",     # tokenization & text mining verbs
  "tm",           # classic corpus & DTM tools
  "SnowballC",    # word stemming
  "text2vec",     # vectorizers, TCM, wordâ€embedding training
  
  ## 3. Modeling & Tuning
  "caret",        # unified train/tune interface
  "glmnet",       # lasso/ridge/logistic regression
  "e1071",        # SVM (libsvm wrapper)
  "kernlab",      # alternative SVM backend
  "nnet",         # feedâ€‘forward neural networks
 
  
  ## 4. Evaluation Metrics
  "pROC",         # ROC curves, AUC calculations
  
  ## 5. Visualization
  "ggplot2",      # core plotting library
  "ggpubr",       # publicationâ€‘ready ggplot helpers
  "wordcloud",    # wordâ€cloud generation
  "RColorBrewer", # qualitative & sequential palettes
  
  ## 6. Reporting & Utilities
  "knitr",        # R Markdown rendering
  
))
```
### Python Packages

```
pip install sentence-transformers numpy
```

## Dataâ€¯&â€¯Model Artifacts

To avoid bloating the repo, we host large precomputed assets on Googleâ€¯Drive. Please download the following files and place them in the corresponding folders: 

**Google Drive Link**: https://drive.google.com/drive/folders/1anm1pPPj4va1_eXv73enVHan13p2OUW3?dmr=1&ec=wgc-drive-globalnav-goto

| Data                                      | Destination                              | Googleâ€¯Drive Link                                 |
|-------------------------------------------|-------------------------------------------|---------------------------------------------------|
| Raw dataset (`train.csv`)                 | `data/train.csv`                          | [Download](https://drive.google.com/file/d/1T_JWDqKeH83HItuDhMj3ya_6P9Lsiq6g/view?usp=drive_link) |
| PCA components of Bagâ€‘ofâ€‘Words            | `data/pca_bag_words.RData`                | [Download](https://drive.google.com/file/d/1H2HJ6teyEn6tlqS0lhxm6sXl3dP0AkgX/view?usp=drive_link) |
| PCA components of TFâ€‘IDF                  | `data/pca_tfidf.RData`                    | [Download](https://drive.google.com/file/d/15YOAf3DzVtzBLI_NWs4LhfMI-jlVuj-3/view?usp=drive_link) |
| BERT sentence embeddings                  | `data/dtm_bert.npy`                       | [Download](https://drive.google.com/file/d/13yL63KPz0V86grqeeUIWL7Bf0faowXU2/view?usp=drive_link) |

And we have the following selective trained models, because of long time of training.

| Trained model                              | Destination                              | Googleâ€¯Drive Link                                 |
|--------------------------------------------|------------------------------------------|---------------------------------------------------|
| BoWâ€¯Logistic regression                    | `models/bow_logit_model_0.rds`           | [Download](https://drive.google.com/file/d/1ffHjUpe-msWToJS5s6clqdM6BzbBZ6iV/view?usp=drive_link) |
| TFâ€‘IDFâ€¯Logistic regression                 | `models/tfidf_logit_model_0.rds`         | [Download](https://drive.google.com/file/d/1zorYJDzcbh48TmVdneyR7qeHFpABc_pX/view?usp=drive_link) |
| BoWâ€¯SVM (linear)                           | `models/svm_model.rds`                   | [Download](https://drive.google.com/file/d/1bzIT1ZoZfoDM9zpQ92f9J47-K6fzCc7C/view?usp=drive_link) |
| TFâ€‘IDFâ€¯SVM (linear)                        | `models/svm_model_tfidf.rds`             | [Download](https://drive.google.com/file/d/1H3tWpZXoZRfuUrtEehyJwQY_MMJX7Lm0/view?usp=drive_link) |
| BoWâ€¯+â€¯PCAâ€¯SVM                              | `models/svm_model_pca.rds`               | [Download](https://drive.google.com/file/d/1g5ZNQIsBXTXDpAtjozyt3bem4KkUGyrp/view?usp=drive_link) |
| TFâ€‘IDFâ€¯+â€¯PCAâ€¯SVM                           | `models/svm_model_pca_tfidf.rds`         | [Download](https://drive.google.com/file/d/1HmVkx4M0RX9A18cxS_xwkH3HB2ZANYXy/view?usp=drive_link) |
| GloVeâ€¯SVM (radial)                         | `models/svm_model_embedding.rds`         | [Download](https://drive.google.com/file/d/1SGS356lDSmgyuP3Qz-mSuZJOvDScfuWB/view?usp=drive_link) |
| BERTâ€¯SVM (radial)                          | `models/svm_model_bert.rds`              | [Download](https://drive.google.com/file/d/1OjdNhcMOjmnhktOEgBFWVBLs0ideW7VQ/view?usp=drive_link) |
| BoWâ€¯+â€¯PCAâ€¯Neuralâ€‘Net                       | `models/neural_network_bag_of_words.RData` | [Download](https://drive.google.com/file/d/1rndkwjtFJ_nFREumKBjXVQn2XAChik_u/view?usp=drive_link) |
| TFâ€‘IDFâ€¯+â€¯PCAâ€¯Neuralâ€‘Net                    | `models/neural_network_tfidf.RData`      | [Download](https://drive.google.com/file/d/1Tq-Le2COeL3IXGfr10TXxs8d4chWYo72/view?usp=drive_link) |
| GloVeâ€¯Neuralâ€‘Net                           | `models/neural_network_glove.RData`      | [Download](https://drive.google.com/file/d/1CnQDfsMsIMPq204Qhn9ntknp52EG956l/view?usp=drive_link) |
| BERTâ€¯Neuralâ€‘Net                            | `models/neural_network_bert.RData`       | [Download](https://drive.google.com/file/d/1IjOC5Fqv7dDk5ASiys0TG4z9_vDUdKL1/view?usp=drive_link) |

After downloading, your directory should look like:

```
.
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ train.csv                          # Raw dataset  
â”‚   â”œâ”€â”€ pca_bag_words.RData                # BoW + PCA components  
â”‚   â”œâ”€â”€ pca_tfidf.RData                    # TFâ€‘IDF + PCA components  
â”‚   â””â”€â”€ dtm_bert.npy                       # BERT sentence embeddings  
â”‚
â”œâ”€â”€ models/  
â”‚   â”œâ”€â”€ bow_logit_model_0.rds              # BoW + Logistic Regression  
â”‚   â”œâ”€â”€ tfidf_logit_model_0.rds            # TFâ€‘IDF + Logistic Regression  
â”‚   â”œâ”€â”€ svm_model.rds                      # BoW + SVM (linear)  
â”‚   â”œâ”€â”€ svm_model_tfidf.rds                # TFâ€‘IDF + SVM (linear)  
â”‚   â”œâ”€â”€ svm_model_pca.rds                  # BoW + PCA + SVM  
â”‚   â”œâ”€â”€ svm_model_pca_tfidf.rds            # TFâ€‘IDF + PCA + SVM  
â”‚   â”œâ”€â”€ svm_model_embedding.rds            # GloVe + SVM (radial)  
â”‚   â”œâ”€â”€ svm_model_bert.rds                 # BERT + SVM (radial)  
â”‚   â”œâ”€â”€ neural_network_bag_of_words.RData  # BoW + PCA + Neural Net  
â”‚   â”œâ”€â”€ neural_network_tfidf.RData         # TFâ€‘IDF + PCA + Neural Net  
â”‚   â”œâ”€â”€ neural_network_glove.RData         # GloVe + Neural Net  
â”‚   â””â”€â”€ neural_network_bert.RData          # BERT + Neural Net  
â”‚
â”œâ”€â”€ R/                                     # R scripts by stage (optional)  
â”‚   â”œâ”€â”€ 01_preprocessing.R  
â”‚   â”œâ”€â”€ 02_feature_engineering.R  
â”‚   â”œâ”€â”€ 03_modeling.R  
â”‚   â””â”€â”€ 04_evaluation.R  
â”‚
â”œâ”€â”€ docs/                                  # Project documentation  
â”‚   â”œâ”€â”€ presentation_slides.pdf            # Presentation slides (PDF)  
â”‚   â””â”€â”€ report_paper.pdf                   # Final project report paper (PDF)  
â”‚
â”œâ”€â”€ disaster_detection_source_code.html    # Project Source Code (html)
â”œâ”€â”€ disaster_detection_source_code.rmd     # Project Source Code (rmd)
â”‚
â”œâ”€â”€ README.md                              # This file   
â””â”€â”€ .gitignore  
```

## Authors
  - Lulin Yang (email: luy30@pitt.edu)   
  - Wendi Li (email: wel242@pitt.edu)  
  - Xiaoxuan Qin (email: xiq33@pitt.edu)  

## Acknowledgments
- We gratefully acknowledge **Kaggle** for hosting the â€œNatural Language Processing with Disaster Tweetsâ€ competition and providing the labeled dataset that forms the basis of this study  [oai_citation_attribution:0â€¡Kaggle](https://www.kaggle.com/competitions/nlp-getting-started/overview/evaluation?utm_source=chatgpt.com).  
- We thank the **Stanford NLP Group** for releasing the preâ€‘trained GloVe word embeddings, which we leveraged for semantic text representation  [oai_citation_attribution:1â€¡Wikipedia](https://en.wikipedia.org/wiki/GloVe?utm_source=chatgpt.com).  



## License
[MIT](https://choosealicense.com/licenses/mit/)
